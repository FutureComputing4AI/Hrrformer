import time
import optax
import flax.linen as nn
from malware_utils import *
from dataset import load_dataset
from flax.training import train_state
from HRR.with_flax import Binding, Unbinding, CosineSimilarity

from flax import jax_utils
from flax.training.common_utils import shard, shard_prng_key


class MHAttention(nn.Module):
    features: int
    heads: int = 8

    def setup(self):
        self.binding = Binding()
        self.unbinding = Unbinding()
        self.similarity = CosineSimilarity()

    @nn.compact
    def __call__(self, query, key, value, mask=None):
        dense_q = nn.Dense(features=self.features)
        dense_k = nn.Dense(features=self.features)
        dense_v = nn.Dense(features=self.features)
        dense_o = nn.Dense(features=self.features)

        q = dense_q(query)  # (B, T, H)
        k = dense_k(key)  # (B, T, H)
        v = dense_v(value)  # (B, T, H)

        q = split(q, self.heads)  # (B, h, T, H')
        k = split(k, self.heads)  # (B, h, T, H')
        v = split(v, self.heads)  # (B, h, T, H')

        bind = self.binding(k, v, axis=-1)  # (B, h, T, H')
        bind = np.sum(bind, axis=-2, keepdims=True)  # (B, h, 1, H')

        vp = self.unbinding(bind, q, axis=-1)  # (B, h, T, H')
        scale = self.similarity(v, vp, axis=-1, keepdims=True)  # (B, h, T, 1)

        scale = scale + (1. - mask) * (-1e9)
        weight = nn.softmax(scale, axis=-2)
        weighted_value = weight * v

        output = merge(weighted_value)
        output = dense_o(output)
        return output


class FeedForwardLayer(nn.Module):
    features: int
    dropout_rate: float = 0.0
    training: bool = False

    @nn.compact
    def __call__(self, inputs):
        x = nn.Dense(features=self.features)(inputs)
        x = nn.relu(x)
        x = nn.Dense(features=inputs.shape[-1])(x)
        x = nn.Dropout(rate=self.dropout_rate)(x, deterministic=not self.training)
        return x


class Encoder(nn.Module):
    features: int
    dropout_rate: float
    training: bool = False

    @nn.compact
    def __call__(self, inputs, mask=None):
        lnx = nn.LayerNorm()(inputs)
        attention = MHAttention(features=inputs.shape[-1])(query=lnx, key=lnx, value=lnx, mask=mask)
        attention = nn.Dropout(self.dropout_rate, deterministic=not self.training)(attention)
        x = inputs + attention

        lnx = nn.LayerNorm()(x)
        ffn = FeedForwardLayer(self.features, self.dropout_rate, training=self.training)(lnx)
        outputs = x + ffn
        return outputs


class Embedding(nn.Module):
    vocab_size: int
    embed_size: int
    max_seq_len: int

    def setup(self):
        self.positions = np.arange(start=0, stop=self.max_seq_len, step=1)[np.newaxis, :]

    @nn.compact
    def __call__(self, inputs):
        word_embedding = nn.Embed(self.vocab_size, self.embed_size)(inputs)
        position_embedding = nn.Embed(self.max_seq_len, self.embed_size)(self.positions)
        return word_embedding + position_embedding


class Network(nn.Module):
    features: int
    vocab_size: int
    embed_size: int
    max_seq_len: int
    nlayers: int
    output_size: int
    dropout_rate: float
    training: bool

    @nn.compact
    def __call__(self, encoder_input):
        encoder_input = encoder_input.astype('int32')  # (B, T)

        en_mask = np.where(encoder_input > 0, 1., 0.)
        en_mask = en_mask[:, np.newaxis, :, np.newaxis]

        # embedding
        x = Embedding(vocab_size=self.vocab_size,
                      embed_size=self.embed_size,
                      max_seq_len=self.max_seq_len)(encoder_input)

        # class token
        token = self.param('cls_token', nn.initializers.lecun_normal(), (1, 1, self.embed_size))
        token = np.repeat(token, x.shape[0], axis=0)
        x = np.concatenate([token, x], axis=1)

        # adjust mask
        en_mask = np.concatenate([np.ones((en_mask.shape[0], 1, 1, 1)), en_mask], axis=-2)

        x = nn.Dropout(self.dropout_rate, deterministic=not self.training)(x)

        for i in range(self.nlayers):
            x = Encoder(features=self.features,
                        dropout_rate=self.dropout_rate,
                        training=self.training)(x, mask=en_mask)

        # output
        x = x[:, 0]

        x = nn.Dense(features=x.shape[-1])(x)
        x = nn.relu(x)

        output = nn.Dense(self.output_size)(x)
        output = nn.softmax(output, axis=-1)
        return output


def initialize_model(model, input_size, init_rngs):
    init_inputs = np.ones([1, input_size])
    variables = model.init(init_rngs, init_inputs)['params']
    return variables


def train_step(state, batch, rngs):
    """ train one step """
    y_true = batch[1]

    def loss_fn(params):
        y_pred = state.apply_fn({'params': params}, batch[0], rngs=rngs)
        loss = cross_entropy_loss(y_true=y_true, y_pred=y_pred)
        return loss, y_pred

    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
    (loss, y_pred), grads = grad_fn(state.params)
    grads = jax.lax.pmean(grads, "batch")
    grads = grad_check(grads)
    state = state.apply_gradients(grads=grads)
    acc = accuracy(y_true=y_true, y_pred=y_pred)
    metrics = {'loss': loss, 'accuracy': acc}
    return state, metrics


def predict(state, batch, rngs):
    y_true = batch[1]
    y_pred = state.apply_fn({'params': state.params}, batch[0], rngs=rngs)
    loss = cross_entropy_loss(y_true=y_true, y_pred=y_pred)
    acc = accuracy(y_true=y_true, y_pred=y_pred)
    metrics = {'loss': loss, 'accuracy': acc}
    return metrics


def train(batch_size, max_seq_len, features=512, embed_size=256, lr=1e-3, epochs=10):
    batch_size = batch_size * jax.device_count()
    vocab_size = 256 + 1
    n_layer = 1
    output_size = 2
    dropout_rate = 0.1
    name = 'hrrformer'

    print('total batch size:', batch_size, 'max seq len:', max_seq_len)

    # load dataset
    train_loader, test_loader = load_dataset(batch_size=batch_size,
                                             max_seq_len=max_seq_len,
                                             shuffle=True,
                                             num_workers=0)

    # build and initialize network
    train_model = Network(features=features,
                          vocab_size=vocab_size,
                          embed_size=embed_size,
                          max_seq_len=max_seq_len,
                          nlayers=n_layer,
                          output_size=output_size,
                          dropout_rate=dropout_rate,
                          training=True)

    test_model = Network(features=features,
                         vocab_size=vocab_size,
                         embed_size=embed_size,
                         max_seq_len=max_seq_len,
                         nlayers=n_layer,
                         output_size=output_size,
                         dropout_rate=dropout_rate,
                         training=False)

    p_key_next, p_key = jax.random.split(jax.random.PRNGKey(0))
    d_key_next, d_key = jax.random.split(jax.random.PRNGKey(0))
    init_rngs = {'params': p_key, 'dropout': d_key}

    params = initialize_model(model=train_model, input_size=max_seq_len, init_rngs=init_rngs)

    # optimizer and scheduler
    steps = 800_000 // batch_size
    scheduler = optax.exponential_decay(init_value=lr,
                                        transition_steps=steps,
                                        decay_rate=.85,
                                        transition_begin=1,
                                        end_value=1e-5)

    tx = optax.adam(learning_rate=scheduler)
    state = train_state.TrainState.create(apply_fn=train_model.apply, params=params, tx=tx)
    # state = load_model(state, f'weights/{name}_multi_{n_layer}_{max_seq_len}.h5')
    state = jax_utils.replicate(state)

    # train
    history = []
    train_loss, train_acc = [], []
    test_loss, test_acc = [], []

    form = 'Epoch {0:>3d}/' + str(epochs) + ', train loss: {1:>8.6f}, train accuracy: {2:>5.2f}%, '
    form += 'test loss: {3:>8.6f}, test accuracy: {4:>5.2f}%, etc: {5:>.2f}s'

    for epoch in range(1, epochs + 1):
        train_loss_batch, train_acc_batch = [], []
        state = state.replace(apply_fn=train_model.apply)

        tic1 = time.time()
        for x_train, y_train in train_loader:
            p_key_next, p_key = jax.random.split(p_key_next)
            d_key_next, d_key = jax.random.split(d_key_next)
            rngs = {'params': shard_prng_key(p_key), 'dropout': shard_prng_key(d_key)}

            batch = [x_train, y_train]
            batch = shard(batch)

            state, metrics = jax.pmap(train_step, axis_name="batch", donate_argnums=(0,))(state, batch, rngs)

            train_loss_batch.append(metrics['loss'].mean())
            train_acc_batch.append(metrics['accuracy'].mean())

        toc1 = time.time()
        train_loss.append(sum(train_loss_batch) / len(train_loss_batch))
        train_acc.append(sum(train_acc_batch) / len(train_acc_batch))

        # test
        test_loss_batch, test_acc_batch = [], []
        state = state.replace(apply_fn=test_model.apply)

        tic2 = time.time()
        for x_test, y_test in test_loader:
            p_key_next, p_key = jax.random.split(p_key_next)
            d_key_next, d_key = jax.random.split(d_key_next)
            rngs = {'params': shard_prng_key(p_key), 'dropout': shard_prng_key(d_key)}

            test_batch = [x_test, y_test]
            test_batch = shard(test_batch)

            metrics = jax.pmap(predict, axis_name="batch")(state, test_batch, rngs)

            test_loss_batch.append(metrics['loss'].mean())
            test_acc_batch.append(metrics['accuracy'].mean())

        toc2 = time.time()
        test_loss.append(sum(test_loss_batch) / len(test_loss_batch))
        test_acc.append(sum(test_acc_batch) / len(test_acc_batch))

        etc = (toc1 - tic1) + (toc2 - tic2)
        history.append(form.format(epoch, train_loss[-1], train_acc[-1], test_loss[-1], test_acc[-1], etc))
        print(history[-1])

    state = jax_utils.unreplicate(state)
    save_model(state, f'weights/{name}_multi_{n_layer}_{max_seq_len}.h5')
    save_history(f'weights/{name}_multi_{n_layer}_{max_seq_len}.csv', history=history)


if __name__ == '__main__':
    from math import log2

    seq = [2 ** i for i in range(8, 20)]
    for seq_len in seq:
        batch = max(2 ** int(16 - log2(seq_len)), 1)
        train(batch_size=batch, max_seq_len=seq_len, epochs=10)
